{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kz1hqb2oWg96"
   },
   "source": [
    "# Инструменты для работы с языком"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iXhKw6r2Wg97"
   },
   "source": [
    "... или зачем нужна предобработка.\n",
    "\n",
    "Раньше мы смотрели на светлую сторону анализа данных - построение моделей. Теперь попробуем глубже посмотреть на часть про предобработку данных. Задача предобработки особенно актуальна, если мы имеем дело с текстами."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvOWdHcUWg98"
   },
   "source": [
    "## Задача: классификация твитов по тональности\n",
    "\n",
    "У нас есть выборка из твитов.\n",
    "Нам известна эмоциональная окраска каждого твита из выборки: положительная или отрицательная. Задача состоит в построении модели, которая по тексту твита предсказывает его эмоциональную окраску.\n",
    "\n",
    "Классификацию по тональности используют в рекомендательных системах, чтобы понять, понравилось ли людям кафе, кино, etc.\n",
    "\n",
    "Скачиваем выборку ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYvTkhGnWg98"
   },
   "outputs": [],
   "source": [
    "# если у вас линукс / мак / collab или ещё какая-то среда, в которой работает wget, можно скачать данные так:\n",
    "%%capture\n",
    "!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv\n",
    "!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IX-AeH8RWg9-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd # библиотека для удобной работы с датафреймами\n",
    "import numpy as np # библиотека для удобной работы со списками и матрицами\n",
    "\n",
    "# библиотека, где реализованы основные алгоритмы машинного обучения\n",
    "from sklearn.metrics import * \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "executionInfo": {
     "elapsed": 3327,
     "status": "ok",
     "timestamp": 1595572083245,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "ZtGDKFvQYEDO",
    "outputId": "4d6d343e-d307-4249-e79e-3342e381b4ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"408906692374446080\";\"1386325927\";\"pleease_shut_up\";\"@first_timee хоть я и школота, но поверь, у нас то же самое :D общество профилирующий предмет типа)\";\"1\";\"0\";\"0\";\"0\";\"7569\";\"62\";\"61\";\"0\"\r\n",
      "\"408906692693221377\";\"1386325927\";\"alinakirpicheva\";\"Да, все-таки он немного похож на него. Но мой мальчик все равно лучше:D\";\"1\";\"0\";\"0\";\"0\";\"11825\";\"59\";\"31\";\"2\"\r\n",
      "\"408906695083954177\";\"1386325927\";\"EvgeshaRe\";\"RT @KatiaCheh: Ну ты идиотка) я испугалась за тебя!!!\";\"1\";\"0\";\"1\";\"0\";\"1273\";\"26\";\"27\";\"0\"\r\n",
      "\"408906695356973056\";\"1386325927\";\"ikonnikova_21\";\"RT @digger2912: \"\"Кто то в углу сидит и погибает от голода, а мы ещё 2 порции взяли, хотя уже и так жрать не хотим\"\" :DD http://t.co/GqG6iuE2…\";\"1\";\"0\";\"1\";\"0\";\"1549\";\"19\";\"17\";\"0\"\r\n",
      "\"408906761416867842\";\"1386325943\";\"JumpyAlex\";\"@irina_dyshkant Вот что значит страшилка :D\n",
      "Но блин,посмотрев все части,у тебя создастся ощущение,что авторы курили что-то :D\";\"1\";\"0\";\"0\";\"0\";\"597\";\"16\";\"23\";\"1\"\r\n",
      "\"408906761769598976\";\"1386325943\";\"JustinB94262583\";\"ну любишь или нет? — Я не знаю кто ты бля:D http://t.co/brf9eNg1U6\";\"1\";\"0\";\"0\";\"0\";\"40\";\"6\";\"16\";\"0\"\r\n",
      "\"408906762436481024\";\"1386325943\";\"twinkleAYO\";\"RT @SpoonLamer: Ох,900 :D ну это конечно же @twinkleAYO . Чтобы у нее было много друзей, ведь она такая мимими &lt;3\";\"1\";\"0\";\"1\";\"0\";\"5169\";\"58\";\"43\";\"2\"\r\n",
      "\"408906764114206720\";\"1386325944\";\"pycalyruhog\";\"RT @veregijytaqo: У тебя есть ухажёр? Нет - мои уши не кто не жрёт :D\";\"1\";\"0\";\"2\";\"0\";\"393\";\"112\";\"153\";\"0\"\r\n",
      "\"408906764608749568\";\"1386325944\";\"grishintv\";\"Поприветствуем моего нового читателя @Alexey1789 ;)\";\"1\";\"0\";\"0\";\"0\";\"5872\";\"1387\";\"1431\";\"12\"\r\n"
     ]
    }
   ],
   "source": [
    "!head positive.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVsSSqFKbAWL"
   },
   "source": [
    "Откроем файлы и создадим массив из текстов и правильных меток для твитов.\n",
    "Сначала идут положительные твиты, потом отрицательные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBrSNCKVWg9_"
   },
   "outputs": [],
   "source": [
    "# загружаем положительные твиты\n",
    "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])\n",
    "positive['label'] = ['positive'] * len(positive) # устанавливаем метки\n",
    "\n",
    "# загружаем отрицательные твиты\n",
    "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
    "negative['label'] = ['negative'] * len(negative) # устанавливаем метки\n",
    "\n",
    "# соединяем вместе\n",
    "df = positive.append(negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfjWsULtaGtf"
   },
   "source": [
    "Посмотрим на полученные данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 2712,
     "status": "ok",
     "timestamp": 1595572083673,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "aqudQEvUWg-D",
    "outputId": "d267f796-b4de-45e9-a350-fd275dc81769"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15931</th>\n",
       "      <td>RT @Blawar_1337: Теперь у нас с @Wake_UA появи...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59532</th>\n",
       "      <td>с днём рождения зайка*))) ухх погуляем мы сего...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47185</th>\n",
       "      <td>RT @Shumkova0406199: @ann_safina Вов вов вов А...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42002</th>\n",
       "      <td>Надо выдернуть звуковую дорожку из \"Доктора Ка...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109035</th>\n",
       "      <td>@_hassliebe_ может все таки на этой неделе вер...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     label\n",
       "15931   RT @Blawar_1337: Теперь у нас с @Wake_UA появи...  positive\n",
       "59532   с днём рождения зайка*))) ухх погуляем мы сего...  positive\n",
       "47185   RT @Shumkova0406199: @ann_safina Вов вов вов А...  negative\n",
       "42002   Надо выдернуть звуковую дорожку из \"Доктора Ка...  positive\n",
       "109035  @_hassliebe_ может все таки на этой неделе вер...  negative"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5, random_state=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ubV1-2JjaZuC"
   },
   "source": [
    "Разбиваем данные на обучающую и тестовую выборки с помощью функции ```train_test_split()``` из **sklearn**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8RscvM_TWg-F"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zpq4QOU5Wg-H"
   },
   "source": [
    "## Baseline: классификация необработанных n-грамм\n",
    "\n",
    "* Сейчас мы попробуем получить преобразование предложений в численный вектор, с которым может работать стандартный алгоритм машинного обучения, такой как логистическая регрессия. \n",
    "* Для этого нам понадобится познакомиться с понятием n-gram - самых мелких элементов предложения, с которыми можно работать. \n",
    "* Подсчитав количество этих n-грам в предложениях, мы получим искомые численные представления."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_7DyyXRWg-K"
   },
   "source": [
    "## Что такое n-граммы:\n",
    "\n",
    "Самые мелкие структуры языка, с которыми мы работаем, называются **n-граммами**.\n",
    "У n-граммы есть параметр n - количество слов, которые попадают в такое представление текста.\n",
    "* Если n = 1 - то мы смотрим на то, сколько раз каждое слово встретилось в тексте. Получаем _униграммы_\n",
    "* Если n = 2 - то мы смотрим на то, сколько раз каждая пара подряд идущих слов, встретилась в тексте. Получаем _биграммы_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quiUoyqNb3WA"
   },
   "source": [
    "Функция для работы с n-граммами реализована в библиотке **nltk** (Natural Language ToolKit), импортируем эту функцию: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcrWxBzzWg-K"
   },
   "outputs": [],
   "source": [
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ib-zYTvfQq5"
   },
   "source": [
    "Прежде чем получать n-граммы, нужно разделить предложение на отдельные слова.  Для этого используем метод ```split()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1052,
     "status": "ok",
     "timestamp": 1595572083675,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "A2Ql8Em4Wg-N",
    "outputId": "94b828c7-4276-4bf2-a172-98f03ee4e786"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Если', 'б', 'мне', 'платили', 'каждый', 'раз']"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'Если б мне платили каждый раз'.split()\n",
    "sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6V5P2Jcc4Oy"
   },
   "source": [
    "Чтобы получить n-грамму для такой последовательности, используем функцию ```ngrams()```. \n",
    "\n",
    "На вход передается два параметра:\n",
    "* лист с разделенным на отдельные слова предложением (у нас он хранится в переменной ```sent```);\n",
    "* параметр n, определяющий, какой тип n-грамм мы хотим получить.\n",
    "\n",
    "\n",
    "Чтобы полученный объект отобразить, делаем из него ```list```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1595572083675,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "F9oqpykUc5e9",
    "outputId": "38c2dd15-5830-4da0-c37a-3d72f1170dd7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если',), ('б',), ('мне',), ('платили',), ('каждый',), ('раз',)]"
      ]
     },
     "execution_count": 57,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sentence, 1)) # униграммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GZKRhRlxfoj4"
   },
   "source": [
    "Аналогично мы можем получить биграммы - для этого заменяем параметр **n** в функции **ngrams** с 1 на 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1595572083991,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "Bzl6t5dpWg-P",
    "outputId": "02d05cbb-e64c-4c3c-d5ae-681c63e0d0cc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б'),\n",
       " ('б', 'мне'),\n",
       " ('мне', 'платили'),\n",
       " ('платили', 'каждый'),\n",
       " ('каждый', 'раз')]"
      ]
     },
     "execution_count": 58,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sentence, 2)) # биграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "executionInfo": {
     "elapsed": 625,
     "status": "ok",
     "timestamp": 1595572084259,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "nCkkFzWLWg-R",
    "outputId": "34818c20-192b-4786-fcc6-149ea0017726"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б', 'мне'),\n",
       " ('б', 'мне', 'платили'),\n",
       " ('мне', 'платили', 'каждый'),\n",
       " ('платили', 'каждый', 'раз')]"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sentence, 3)) # триграммы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1595572084260,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "GygS6_fJWg-S",
    "outputId": "17d4a687-d6ac-4982-c444-aa5a77de9337"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Если', 'б', 'мне', 'платили', 'каждый'),\n",
       " ('б', 'мне', 'платили', 'каждый', 'раз')]"
      ]
     },
     "execution_count": 60,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ngrams(sentence, 5)) # ... пентаграммы?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JewKs4XU-so"
   },
   "source": [
    "### Векторизаторы\n",
    "\n",
    "Векторизатор преобразует слово или набор слов в числовой вектор, понятный алгоритму машинного обучения, который привык работать с числовыми табличными данными.\n",
    "\n",
    "Ниже - пример преобразования слов в двумерных вектор, каждому слову соответствует точка на плоскости.\n",
    "\n",
    "<a href=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\n",
    "\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\" \n",
    "alt=\"IMAGE ALT TEXT HERE\" width=\"600\" border=\"0\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5hiNv2eVAc-"
   },
   "source": [
    "На начальном этапе нам будет достаточно тех инструментов, которые уже есть в знакомой нам библиотеке **sklearn**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cPplZnxeVEBR"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression # можно заменить на любимый классификатор\n",
    "from sklearn.feature_extraction.text import CountVectorizer # модель \"мешка слов\", см. далее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBN16KYZWg-U"
   },
   "source": [
    "Самый простой способ извлечь признаки из текстовых данных -- векторизаторы: `CountVectorizer` и `TfidfVectorizer`\n",
    "\n",
    "Объект `CountVectorizer` делает следующую вещь:\n",
    "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` -- количество слов или n-грам во всём корпусе\n",
    "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ\n",
    "\n",
    "<a href=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\n",
    "\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1jHmkrGZTMawM46Yzxh243Ur1y5pYKzrl\" \n",
    "alt=\"IMAGE ALT TEXT HERE\" width=\"600\" border=\"0\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oklbwY_vWg-X"
   },
   "source": [
    "На рисунке пример векторизации для униграмм, но можно использовать любые n-граммы. Для этого у объекта ```CountVectorizer()``` есть параметр **ngram_range**, который отвечает за то, какие n-граммы мы используем в качестве признаов:<br/>\n",
    "ngram_range=(1, 1) -- униграммы<br/>\n",
    "ngram_range=(3, 3) -- триграммы<br/>\n",
    "ngram_range=(1, 3) -- униграммы, биграммы и триграммы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BjJmHbOvVmln"
   },
   "source": [
    "<a href=\"https://drive.google.com/uc?id=1ODNVK0fdLTX4nv6ob55ciUe37d1pio-D\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1ODNVK0fdLTX4nv6ob55ciUe37d1pio-D\" \n",
    "alt=\"IMAGE ALT TEXT HERE\" width=\"800\" border=\"0\" /></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLTEzNWxrx5X"
   },
   "source": [
    "Инициализируем ```CountVectorizer()```, указав в качестве признаков униграммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hWKQcLfBWg-W"
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-gFUNm6huAz"
   },
   "source": [
    "После инициализации _vectorizer_ можно обучить на наших данных. \n",
    "\n",
    "Для обучения используем обучающую выборку ```x_train```, но в отличие от классификатора мы используем метод ```fit_transform()```: сначала обучаем наш векторизатор, а потом сразу применяем его к нашему набору данных. Это похоже на то, как мы работали с label encoderом и one-hot-encoderом.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TkJESwGfhq4v"
   },
   "outputs": [],
   "source": [
    "vectorized_x_train = vectorizer.fit_transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPJDPt7xvWKe"
   },
   "source": [
    "Так как результат не зависит от порядка слов в текстах, то говорят, что такая модель представления текстов в виде векторов получается из *гипотезы представления текста как мешка слов*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyT1Kz6Ppt8n"
   },
   "source": [
    "В vectorizer.vocabulary_ лежит словарь, отображение слов в их индексы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "executionInfo": {
     "elapsed": 1849,
     "status": "ok",
     "timestamp": 1595572093192,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "72knqTvCWg-Y",
    "outputId": "2b67a368-df3b-45ff-c17a-3cf25252acac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vitasanko', 89401),\n",
       " ('заутра', 136852),\n",
       " ('теоретическая', 221018),\n",
       " ('часть', 236221),\n",
       " ('послезавтра', 189994),\n",
       " ('практическая', 191963),\n",
       " ('avmorgun', 15601),\n",
       " ('во', 113459),\n",
       " ('льду', 154217),\n",
       " ('красная', 148932)]"
      ]
     },
     "execution_count": 65,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(vectorizer.vocabulary_.items())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zdKqMyRKjA-p"
   },
   "source": [
    "В нашей выборке 170125 текстов (твитов), в них встречается 243760 разных слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 11667,
     "status": "ok",
     "timestamp": 1595571716091,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "IjOD7nrsi2fc",
    "outputId": "5141f2d9-956c-49cc-fda4-8ed311ee8b04"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(170125, 243742)"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorized_x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7geX_YHDjG9v"
   },
   "source": [
    "Так как теперь у нас есть **численное представление** и набор входных признаков, то мы можем обучить модель логистической регрессии (или любую другую из тех, на которые мы смотрели раньше, например, случайный лес)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "executionInfo": {
     "elapsed": 40995,
     "status": "ok",
     "timestamp": 1595571745425,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "DCdROyxsWg-b",
    "outputId": "96fb55d1-fac0-4ded-b006-517701d0bdcb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(random_state=42, max_iter=1000) # фиксируем random_state для воспроизводимости результатов\n",
    "clf.fit(vectorized_x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Z9iIlzOjVUF"
   },
   "source": [
    "С тестовыми данными нужно проделать то же самое, что и с данными для обучения: сделать из текстов вектора, которые можно передавать в классификатор для прогноза класса объекта. \n",
    "\n",
    "У нас уже есть обученный векторизатор ```vectorizer```, поэтому используем метод ```transform()``` (просто применить его), а не ```fit_transform``` (обучить и применить)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2_AW8FpjWHQ"
   },
   "outputs": [],
   "source": [
    "vectorized_x_test = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahCRPAeWjtcl"
   },
   "source": [
    "Как раньше, для получения прогноза у обученного классификатора используем метод ```predict()```.\n",
    "\n",
    "С помощью функции ```classification_report()```, которая считает сразу несколько метрик качества классификации, посмотрим на то, насколько хорошо мы предсказываем положительную или отрицательную тональность твита ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "executionInfo": {
     "elapsed": 43650,
     "status": "ok",
     "timestamp": 1595571748094,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "juvxbzinWg-d",
    "outputId": "1d876772-e00b-4788-8a22-a365957b1f1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.77      0.76     28145\n",
      "    positive       0.77      0.76      0.77     28564\n",
      "\n",
      "    accuracy                           0.77     56709\n",
      "   macro avg       0.77      0.77      0.77     56709\n",
      "weighted avg       0.77      0.77      0.77     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = clf.predict(vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yiLk1P_xYQ2"
   },
   "source": [
    "## Бонус*: триграммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mjy5ZPmwWg-j"
   },
   "source": [
    "Попробуем сделать то же самое, используя в качестве признаков триграммы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "executionInfo": {
     "elapsed": 77463,
     "status": "ok",
     "timestamp": 1595571781915,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "hmQHqUpRWg-k",
    "outputId": "1777b480-9cbe-40e4-c2a6-d67fba32cb1b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.72      0.47      0.57     28145\n",
      "    positive       0.61      0.82      0.70     28564\n",
      "\n",
      "    accuracy                           0.64     56709\n",
      "   macro avg       0.66      0.64      0.63     56709\n",
      "weighted avg       0.66      0.64      0.63     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# инициализируем векторайзер \n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "\n",
    "# обучаем его и сразу применяем к x_train\n",
    "trigram_vectorized_x_train = trigram_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "clf.fit(trigram_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторизатор к тестовым данным\n",
    "trigram_vectorized_x_test = trigram_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(trigram_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MonLW7AyWg-m"
   },
   "source": [
    "Как вы думаете, почему в результатах теперь такой разброс по сравнению с униграммами?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VlWxW3e9Wg-m"
   },
   "source": [
    "## Бонус**: TF-IDF векторизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7hCxZRtWg-m"
   },
   "source": [
    "`TfidfVectorizer` делает то же, что и `CountVectorizer`, но в качестве значений выдает **tf-idf** каждого слова.\n",
    "\n",
    "Как считается tf-idf:\n",
    "\n",
    "**TF (term frequency)** – относительная частотность слова в документе:\n",
    "$$ TF(t,d) = \\frac{n_{t}}{\\sum_k n_{k}} $$\n",
    "\n",
    "**IDF (inverse document frequency)** – обратная частота документов, в которых есть это слово:\n",
    "$$ IDF(t, D) = \\mbox{log} \\frac{|D|}{|{d : t \\in d}|} $$\n",
    "\n",
    "Перемножаем их:\n",
    "$$TFIDF(t, d, D) = TF(t,d) \\times IDF(i, D)$$\n",
    "\n",
    "Сакральный смысл: если слово часто встречается в одном документе, но в целом по корпусу встречается в небольшом \n",
    "количестве документов, у него высокий TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fv7DfTkJWg-n"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02f_zZm14PHM"
   },
   "source": [
    "Действуем аналогично, как с ```CountVectorizer()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "executionInfo": {
     "elapsed": 203173,
     "status": "ok",
     "timestamp": 1595571907637,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "-rMYiobAWg-p",
    "outputId": "e7782d5f-d23e-4515-ddac-b80de2018e49"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.73      0.77      0.75     28145\n",
      "    positive       0.76      0.71      0.74     28564\n",
      "\n",
      "    accuracy                           0.74     56709\n",
      "   macro avg       0.75      0.74      0.74     56709\n",
      "weighted avg       0.75      0.74      0.74     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# инициализируем векторизатор, в качестве переменных используем униграммы\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 5))\n",
    "\n",
    "# обучаем его и сразу применяем к x_train\n",
    "tfidf_vectorized_x_train = tfidf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(tfidf_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторизатор к тестовым данным\n",
    "tfidf_vectorized_x_test = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(tfidf_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkuods5LWg-q"
   },
   "source": [
    "В этот раз получилось хуже :( Вернёмся к `CountVectorizer()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D39SSh0zWg-r"
   },
   "source": [
    "## Токенизация\n",
    "\n",
    "Токенизировать - значит, поделить текст на части: слова, ключевые слова, фразы, символы и т.д., иными словами **токены**.\n",
    "\n",
    "Самый наивный способ токенизировать текст - разделить с помощью функции `split()`. Но `split` упускает очень много всего, например, не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем, поэтому лучше использовать готовые токенизаторы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoSe08N2Wg-r"
   },
   "outputs": [],
   "source": [
    "import nltk # уже знакомая нам библиотека nltk\n",
    "from nltk.tokenize import word_tokenize # готовый токенизатор библиотеки nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DiDt3L8Y8god"
   },
   "source": [
    "Чтобы использовать токенизатор ```word_tokenize```, нужно сначала скачать данные для nltk о пунктуации и стоп-словах. Это просто требование nltk, поэтому, особо не задумываясь, запустите следующую ячейку:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "executionInfo": {
     "elapsed": 203991,
     "status": "ok",
     "timestamp": 1595571908471,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "gPH3yMcumsdd",
    "outputId": "977038f7-c30f-4676-86cf-042f32f097be"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NfDb8D_9DqD"
   },
   "source": [
    "Применим токенизацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 203985,
     "status": "ok",
     "timestamp": 1595571908471,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "zrJDGpgYWg-4",
    "outputId": "8922d9ce-a200-4f9b-9f00-2ec936638f5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять', ':', '(']"
      ]
     },
     "execution_count": 26,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = 'Но не каждый хочет что-то исправлять:('\n",
    "word_tokenize(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxy7KGZI9bhK"
   },
   "source": [
    "Если использовать просто ```split()```, то грустный смайлик :( не отделяется от слова \"исправлять\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 203980,
     "status": "ok",
     "timestamp": 1595571908472,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "p52dIuSI9W6o",
    "outputId": "80080cf0-6165-4fd2-f4b8-adf2ba709f29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять:(']"
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_702Dg5OWg-5"
   },
   "source": [
    "В nltk вообще есть довольно много токенизаторов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "executionInfo": {
     "elapsed": 203974,
     "status": "ok",
     "timestamp": 1595571908472,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "Ps8oPYoTWg-6",
    "outputId": "edcce30e-ce55-468b-8adc-3daa972e532b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BlanklineTokenizer',\n",
       " 'LineTokenizer',\n",
       " 'MWETokenizer',\n",
       " 'PunktSentenceTokenizer',\n",
       " 'RegexpTokenizer',\n",
       " 'ReppTokenizer',\n",
       " 'SExprTokenizer',\n",
       " 'SpaceTokenizer',\n",
       " 'StanfordSegmenter',\n",
       " 'TabTokenizer',\n",
       " 'TextTilingTokenizer',\n",
       " 'ToktokTokenizer',\n",
       " 'TreebankWordTokenizer',\n",
       " 'TweetTokenizer',\n",
       " 'WhitespaceTokenizer',\n",
       " 'WordPunctTokenizer']"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import tokenize\n",
    "dir(tokenize)[:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lmnGCL5iWg-8"
   },
   "source": [
    "Они умеют выдавать индексы в строке для начала и конца каждого слова-токена:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 203968,
     "status": "ok",
     "timestamp": 1595571908473,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "Jejj5X7QWg-8",
    "outputId": "21214ea3-959d-4b50-fa26-30f606b883fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 2), (3, 5), (6, 12), (13, 18), (19, 25), (26, 38)]"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_tok = tokenize.WhitespaceTokenizer()\n",
    "list(wh_tok.span_tokenize(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a-wf6A1EWg--"
   },
   "source": [
    "Некторые токенизаторы ведут себя специфично:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 203963,
     "status": "ok",
     "timestamp": 1595571908474,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "2REwpHGWWg-_",
    "outputId": "848159bf-477b-415c-cc4f-db2eaefd9265"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', \"n't\", 'stop', 'me']"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tckre90JWg_B"
   },
   "source": [
    "А некоторые -- вообще не для текста на естественном языке:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 203956,
     "status": "ok",
     "timestamp": 1595571908474,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "F1Ml3xtaWg_D",
    "outputId": "2ec44cac-37c0-409c-b00b-32573d0fb6a0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(a (b c))', 'd', 'e', '(f)']"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize.SExprTokenizer().tokenize(\"(a (b c)) d e (f)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bM2kvAo0_b93"
   },
   "source": [
    "**Правильный токенизатор подбирается исходя из требований задачи!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rhVrgkSaWg_K"
   },
   "source": [
    "## Стоп-слова и пунктуация\n",
    "\n",
    "**Стоп-слова** - это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе. Для модели это просто шум. А шум нужно убирать. По аналогичной причине убирают и пунктуацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "executionInfo": {
     "elapsed": 203951,
     "status": "ok",
     "timestamp": 1595571908475,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "Ld-h6WKyWg_K",
    "outputId": "090c952d-213a-434b-8d76-d5ee08f0eece"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "# импортируем стоп-слова из библиотеки nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# посмотрим на стоп-слова для русского языка\n",
    "print(stopwords.words('russian'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ihn2Y_SzBU57"
   },
   "source": [
    "*Знаки* пунктуации лучше импортировать из модуля **String**. В нем хранятся различные наборы констант для работы со строками (пунктуация, алфавит и др.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 203944,
     "status": "ok",
     "timestamp": 1595571908475,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "x64U3FdPWg_M",
    "outputId": "d255d6db-84d9-4517-da65-7b79a23a87b9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 33,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9LGTLaEBwsC"
   },
   "source": [
    "Объединим стоп-слова и знаки пунктуации вместе и запишем в переменную ```noise```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfH5y50wWg_O"
   },
   "outputs": [],
   "source": [
    "noise = stopwords.words('russian') + list(punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t3gweXaWWg_P"
   },
   "source": [
    "Теперь нужно обучать нашу модель с учетом новых знаний про токенизацию и стоп-слова. \n",
    "\n",
    "Для этого мы можем собрать новый векторизатор, передав ему на вход:\n",
    "* какие n-граммы нам нужны, параметр **ngram_range**;\n",
    "* какой токенизатор мы используем, параметр **tokenizer**;\n",
    "* какие у нас стоп-слова, параметр **stop_words**.\n",
    "\n",
    "*Напоминание:* мы используем готовый токенизатор ```word_tokenize```, а стоп-слова хранятся в переменной ```noise```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbXrVeRRuAxx"
   },
   "outputs": [],
   "source": [
    "# инициализируем умный векторайзер \n",
    "smart_vectorizer = CountVectorizer(ngram_range=(1, 1), \n",
    "                                   tokenizer=word_tokenize, \n",
    "                                   stop_words=noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "executionInfo": {
     "elapsed": 252716,
     "status": "ok",
     "timestamp": 1595571957270,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "7Nc6D-nwWg_P",
    "outputId": "22ab3aef-053b-447b-b784-c46f990353db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.80      0.78     28145\n",
      "    positive       0.79      0.76      0.77     28564\n",
      "\n",
      "    accuracy                           0.78     56709\n",
      "   macro avg       0.78      0.78      0.78     56709\n",
      "weighted avg       0.78      0.78      0.78     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# обучаем его и сразу применяем к x_train\n",
    "smart_vectorized_x_train = smart_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(smart_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторайзер к тестовым данным\n",
    "smart_vectorized_x_test = smart_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(smart_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYWB1foQWg_T"
   },
   "source": [
    "Получилось лучше: accuracy выше, а также заметно подрос recall у негативного класса. \n",
    "\n",
    "Что ещё можно сделать?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsRf9T_SWg_U"
   },
   "source": [
    "## Бонус*: Лемматизация\n",
    "\n",
    "**Лемматизация** – это сведение разных форм одного слова к начальной форме – **лемме**. Почему это хорошо?\n",
    "* Во-первых, естественно рассматривать как отдельный признак каждое *слово*, а не каждую его отдельную форму.\n",
    "* Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лематизации выкидываем мы только её.\n",
    "\n",
    "Для русского есть хороших лемматизатор pymorphy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylKZG2MwWg_f"
   },
   "source": [
    "### [Pymorphy](http://pymorphy2.readthedocs.io/en/latest/)\n",
    "Это модуль на питоне, довольно быстрый и с кучей функций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "executionInfo": {
     "elapsed": 257882,
     "status": "ok",
     "timestamp": 1595571962443,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "JcYWYq4BzOon",
    "outputId": "407e7154-21ea-48be-adc7-0b223699be45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymorphy2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
      "\r",
      "\u001b[K     |███████                         | 10kB 24.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 20kB 1.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▎          | 30kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▍   | 40kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 51kB 1.7MB/s \n",
      "\u001b[?25hCollecting dawg-python>=0.7\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
      "Collecting pymorphy2-dicts<3.0,>=2.4\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
      "\u001b[K     |████████████████████████████████| 7.1MB 6.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
      "Installing collected packages: dawg-python, pymorphy2-dicts, pymorphy2\n",
      "Successfully installed dawg-python-0.7.2 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985\n"
     ]
    }
   ],
   "source": [
    "# устанавливаем pymorphy2\n",
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqdT2pmRFn2F"
   },
   "source": [
    "В pymorphy2 для морфологического анализа слов есть ```MorphAnalyzer()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m4nRuUu2Wg_g"
   },
   "outputs": [],
   "source": [
    "from pymorphy2 import MorphAnalyzer\n",
    "pymorphy2_analyzer = MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Egd6KdqzWg_h"
   },
   "source": [
    "pymorphy2 работает с отдельными словами. Если дать ему на вход предложение - он его просто не лемматизирует, т.к. не понимает:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 258231,
     "status": "ok",
     "timestamp": 1595571962807,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "M6hdm1KBFx18",
    "outputId": "cb3efd3a-32a6-41be-bf9a-c2566ea1e103"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Если', 'б', 'мне', 'платили', 'каждый', 'раз']"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['Если', 'б', 'мне', 'платили', 'каждый', 'раз']\n",
    "sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cr1C2beNF3vE"
   },
   "source": [
    "Лемматизируем слово \"платили\" из предложения ```sent``` с помощью метода ```parse()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 258226,
     "status": "ok",
     "timestamp": 1595571962808,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "1Q3zNlPBWg_i",
    "outputId": "ab45454f-9b31-4670-bd17-11fe9508f3e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='платили', tag=OpencorporaTag('VERB,impf,tran plur,past,indc'), normal_form='платить', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'платили', 2368, 10),))]"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana = pymorphy2_analyzer.parse(sent[3])\n",
    "ana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m2O2BL4_GJzq"
   },
   "source": [
    "Выведем его нормальную форму:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 258221,
     "status": "ok",
     "timestamp": 1595571962809,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "7-zp0KZLWg_p",
    "outputId": "9e785098-359a-4082-cc40-30a6c855ebda"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'платить'"
      ]
     },
     "execution_count": 41,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana[0].normal_form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPldMZzPbw_y"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hedBdcYWhAH"
   },
   "source": [
    "## О важности эксплоративного анализа\n",
    "\n",
    "Но иногда пунктуация бывает и не шумом - главное отталкиваться от задачи. Что будет если вообще не убирать пунктуацию?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "executionInfo": {
     "elapsed": 307420,
     "status": "ok",
     "timestamp": 1595572012016,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "XhZMwsY5WhAI",
    "outputId": "c08de556-84e4-4002-c0fa-215fcf7a14f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00     28145\n",
      "    positive       1.00      1.00      1.00     28564\n",
      "\n",
      "    accuracy                           1.00     56709\n",
      "   macro avg       1.00      1.00      1.00     56709\n",
      "weighted avg       1.00      1.00      1.00     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# инициализируем умный векторайзер stop-words НЕ ИСПОЛЬЗУЕМ!\n",
    "alternative_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), \n",
    "                                               tokenizer=word_tokenize)\n",
    "\n",
    "# обучаем его и сразу применяем к x_train\n",
    "alternative_tfidf_vectorized_x_train = alternative_tfidf_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(alternative_tfidf_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторайзер к тестовым данным\n",
    "alternative_tfidf_vectorized_x_test = alternative_tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(alternative_tfidf_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSFebp_1WhAK"
   },
   "source": [
    "Шок! Стоило оставить пунктуацию -- и все метрики равны 1. Как это получилось? Среди неё были очень значимые токены (как вы думаете, какие?)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FEQbCtidWhAP"
   },
   "source": [
    "Посмотрим, как один из супер-значительных токенов справится с классификацией безо всякого машинного обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 179
    },
    "executionInfo": {
     "elapsed": 308958,
     "status": "ok",
     "timestamp": 1595572013561,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "rhUG9qWuWhAQ",
    "outputId": "184494dd-8541-4c02-b92c-40c6be6c8e89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.85      0.92     33055\n",
      "    positive       0.83      1.00      0.91     23654\n",
      "\n",
      "    accuracy                           0.91     56709\n",
      "   macro avg       0.91      0.93      0.91     56709\n",
      "weighted avg       0.93      0.91      0.91     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cool_token = ')'\n",
    "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
    "print(classification_report(pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrqW55jgWhAR"
   },
   "source": [
    "## Символьные n-граммы\n",
    "\n",
    "Теперь в качестве фичей используем, например, униграммы символов. Для этого необходимо установить в ```CountVectorizer()``` параметр ```analyzer = 'char'```, то есть анализировать символы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "executionInfo": {
     "elapsed": 319857,
     "status": "ok",
     "timestamp": 1595572024466,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "o4lNhEmyWhAU",
    "outputId": "8ae6249e-4433-4f9d-9412-8bd287adfb20"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      0.99      0.99     28145\n",
      "    positive       0.99      1.00      0.99     28564\n",
      "\n",
      "    accuracy                           0.99     56709\n",
      "   macro avg       0.99      0.99      0.99     56709\n",
      "weighted avg       0.99      0.99      0.99     56709\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# инициализируем векторайзер для символов\n",
    "char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 1))\n",
    "\n",
    "# обучаем его и сразу применяем к x_train\n",
    "char_vectorized_x_train = char_vectorizer.fit_transform(x_train)\n",
    "\n",
    "# инициализируем и обучаем классификатор\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(char_vectorized_x_train, y_train)\n",
    "\n",
    "# применяем обученный векторайзер к тестовым данным\n",
    "char_vectorized_x_test = char_vectorizer.transform(x_test)\n",
    "\n",
    "# получаем предсказания и выводим информацию о качестве\n",
    "pred = clf.predict(char_vectorized_x_test)\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLMMicsFWhAY"
   },
   "source": [
    "Из предыдущего раздела уже понятно, почему на этих данных точность равна 1. \n",
    "\n",
    "Символьные n-граммы используются, например, для задачи определения языка. Ещё одна замечательная особенность признаков-символов - для них не нужна токенизация и лемматизация, можно использовать такой подход для языков, у которых нет готовых анализаторов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5QYTwyMtWhAZ"
   },
   "source": [
    "## Бонус***: регулярные выражения\n",
    "\n",
    "Регулярные выражения - способ поиска и анализа строк. Например, можно понять, какие даты в наборе строк представлены в формате DD/MM/YYYY, а какие - в других форматах. \n",
    "\n",
    "Или бывает, например, что перед работой с текстом, надо почистить его от своеобразного мусора: упоминаний пользователей, url и так далее.\n",
    "\n",
    "Навык полезный, давайте в нём тоже потренируемся.\n",
    "\n",
    "Для работы с регулярными выражениями есть библиотека **re**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VaUW5S4gWhAb"
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D6aYh7Osl8xr"
   },
   "source": [
    "В регулярных выражениях, кроме привычных символов-букв, есть специальные символы:\n",
    "* **?а** - ноль или один символ **а**\n",
    "* **+а** - один или более символов **а**\n",
    "* **\\*а** - ноль или более символов **а** (не путать с +)\n",
    "* **.** - любое количество любого символа\n",
    "\n",
    "Пример:\n",
    "Выражению \\*a?b. соответствуют последовательности a, ab, abc, aa, aac НО НЕ abb!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7zOFFA3l_KQ"
   },
   "source": [
    "Рассмотрим подробно несколько наиболее полезных функций:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DbJrUpARWhAd"
   },
   "source": [
    "### findall\n",
    "возвращает список всех найденных непересекающихся совпадений.\n",
    "\n",
    "Регулярное выражение **ab+c.**: \n",
    "* **a** - просто символ **a**\n",
    "* **b+** - один или более символов **b**\n",
    "* **c** - просто символ **c**\n",
    "* **.** - любой символ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 319845,
     "status": "ok",
     "timestamp": 1595572024467,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "2athHzKuWhAd",
    "outputId": "472f03c7-9575-4199-d156-306ac688f3c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abcd', 'abca']\n"
     ]
    }
   ],
   "source": [
    "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9FpIw5RWhAf"
   },
   "source": [
    "Вопрос на внимательность: почему нет abcx?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MI18l-l9WhAk"
   },
   "source": [
    "### split\n",
    "разделяет строку по заданному шаблону\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 319834,
     "status": "ok",
     "timestamp": 1595572024468,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "sVKdRoc1WhAl",
    "outputId": "c9ffe7e4-7efc-4e0c-9342-5d0c9479003b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['itsy', ' bitsy', ' teenie', ' weenie']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(',', 'itsy, bitsy, teenie, weenie') \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "10u5efuSWhAm"
   },
   "source": [
    "можно указать максимальное количество разбиений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 319828,
     "status": "ok",
     "timestamp": 1595572024469,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "9U9EQZMwWhAn",
    "outputId": "1b1ab6eb-6015-4075-c519-4f2a81bec9fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['itsy', ' bitsy', ' teenie, weenie']\n"
     ]
    }
   ],
   "source": [
    "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit=2) \n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wrEGqBSWhAr"
   },
   "source": [
    "### sub\n",
    "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
    "\n",
    "параметры: (pattern, repl, string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 319817,
     "status": "ok",
     "timestamp": 1595572024470,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "az3KxKWwWhAr",
    "outputId": "2403a99a-456f-4ea2-b80b-22071719bf81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bbcbbc\n"
     ]
    }
   ],
   "source": [
    "result = re.sub('a', 'b', 'abcabc')\n",
    "print (result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gStgBJy2WhAx"
   },
   "source": [
    "### compile\n",
    "компилирует регулярное выражение в отдельный объект"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 319803,
     "status": "ok",
     "timestamp": 1595572024472,
     "user": {
      "displayName": "Viacheslav Ivanov",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj2nVSPc3MmvxSyrr0vJY2olNGRMXHUnFlFUnxktA=s64",
      "userId": "14095316471752402401"
     },
     "user_tz": -180
    },
    "id": "JstTupisWhAy",
    "outputId": "3abd910c-cc9c-4641-d641-dbc96b5791e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Пример: построение списка всех слов строки:\n",
    "prog = re.compile('[А-Яа-яё\\-]+')\n",
    "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "VlWxW3e9Wg-m",
    "D39SSh0zWg-r",
    "rhVrgkSaWg_K",
    "XsRf9T_SWg_U",
    "ylKZG2MwWg_f",
    "9hedBdcYWhAH",
    "JrqW55jgWhAR",
    "5QYTwyMtWhAZ",
    "DbJrUpARWhAd",
    "MI18l-l9WhAk",
    "1wrEGqBSWhAr",
    "gStgBJy2WhAx"
   ],
   "name": "Копия блокнота \"seminar_1_preprocessing.ipynb\"",
   "provenance": [
    {
     "file_id": "1TtILmuSoWXOYmbTIAQmGaScvuHGWvpsI",
     "timestamp": 1595574229609
    },
    {
     "file_id": "1EdBdyqxLu-WiLmriWNwYl5Ct33JYcEG2",
     "timestamp": 1582113683695
    },
    {
     "file_id": "10_Aehfbxgr3fxXPgI1gM5BTU8yOy-Z4U",
     "timestamp": 1579514615233
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
