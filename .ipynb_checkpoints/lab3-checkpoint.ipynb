{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d6099b-9740-4884-b945-c25bee5b4f59",
   "metadata": {},
   "source": [
    "# Лабораторная работа 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ad25e-a48e-4b58-9896-1d00dfc2c52a",
   "metadata": {},
   "source": [
    "Создать сеть на базе LSTM используя TensorFlow (Keras). \\\n",
    "Сеть должна принимать на вход текстовый файл и на его базе генерировать свою абракадабру. \\\n",
    "Отчет должен содержать кроме кода, обучающий файл и результат генерации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b304a462-ebe7-445f-9fb0-d06127eced99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/alice/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "10da865c-e943-49aa-a08b-bdcb56c0f6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = 'alice_in_wonderland.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5395d61-a643-4259-b1f9-092daa5935d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 148208\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with open(FILE_PATH, 'r', encoding='utf-8') as f:\n",
    "        text = f.read().lower()\n",
    "except:\n",
    "    print(f\"Failed to open file\")\n",
    "\n",
    "print(f'Length: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "721c9ecc-ee4a-4eee-bc0c-e8de3bf0e514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: 34561\n"
     ]
    }
   ],
   "source": [
    "tokens = word_tokenize(text)\n",
    "print(\"Tokens:\", len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae0dcdb2-7963-4f00-bd3e-31a62ac4c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts([tokens])\n",
    "word_index = tokenizer.word_index\n",
    "total_words = len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21abd898-6f8c-48f0-9428-690b6af0e7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_sequences = tokenizer.texts_to_sequences([tokens])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8dfc628-9b0a-4d3b-9f69-f29b6155cbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = []\n",
    "for i in range(SEQUENCE_LENGTH, len(indexed_sequences)):\n",
    "    # Последовательность из N слов (SEQUENCE_LENGTH)\n",
    "    n_gram_sequence = indexed_sequences[i-SEQUENCE_LENGTH:i]\n",
    "    # Добавление к X\n",
    "    input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "61ddf46f-f411-49bc-814e-fffb68e32225",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sequences = np.array(input_sequences)\n",
    "\n",
    "# Разделение на X (входы) и Y (целевое слово)\n",
    "X = input_sequences[:, :-1]\n",
    "y = input_sequences[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "219bbe49-a640-440b-a337-c852ac57a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = to_categorical(y, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ade78a9f-36fd-4bcc-a625-0113cb82bf75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alice/.pyenv/lib/python3.13/site-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "2025-12-14 13:27:37.047221: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Embedding Layer: преобразует целые числа (индексы слов) в плотные векторы\n",
    "model.add(Embedding(total_words, 100, input_length=X.shape[1]))\n",
    "# Слой LSTM: ядро модели для работы с последовательностями\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dropout(0.2))\n",
    "# Dense Layer: выходной слой с активацией softmax для получения вероятности следующего слова\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149b6994-fb89-4bde-8769-62a9ec739b71",
   "metadata": {},
   "source": [
    "Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e2bb89-ba60-4c48-b18c-57052c0bc5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 13:28:22.735427: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:84] Allocation of 364046004 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1077/1077\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 138ms/step - accuracy: 0.0875 - loss: 5.6621\n",
      "Epoch 2/50\n",
      "\u001b[1m  65/1077\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:25\u001b[0m 144ms/step - accuracy: 0.1011 - loss: 5.3137"
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e89cd1-dca7-4b80-ae0c-b0e9657ef2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(seed_text, next_words):\n",
    "    \"\"\"Генерирует текст на основе начальной фразы.\"\"\"\n",
    "    # Преобразование словаря \"индекс -> слово\" для обратного преобразования\n",
    "    reverse_word_index = dict(map(reversed, word_index.items()))\n",
    "    \n",
    "    output_text = seed_text\n",
    "    current_seed = seed_text\n",
    "\n",
    "    for _ in range(next_words):\n",
    "        # Токенизация текущего сида и преобразование в последовательность чисел\n",
    "        token_list = tokenizer.texts_to_sequences([word_tokenize(current_seed)])[0]\n",
    "        # Паддинг до нужной длины\n",
    "        token_list = pad_sequences([token_list], maxlen=X.shape[1], padding='pre')\n",
    "        \n",
    "        # Предсказание вероятностей следующего слова\n",
    "        predicted_probs = model.predict(token_list, verbose=0)[0]\n",
    "        # Выбор индекса слова с максимальной вероятностью (жадный поиск)\n",
    "        # Для более интересной генерации можно использовать сэмплирование\n",
    "        predicted_index = np.argmax(predicted_probs)\n",
    "        \n",
    "        # Преобразование индекса обратно в слово\n",
    "        next_word = reverse_word_index.get(predicted_index, '')\n",
    "        \n",
    "        # Обновление сида и выходного текста\n",
    "        current_seed = current_seed + \" \" + next_word\n",
    "        output_text = output_text + \" \" + next_word\n",
    "        \n",
    "        # Обновление сида для следующей итерации (убираем первое слово)\n",
    "        current_seed_tokens = current_seed.split()\n",
    "        current_seed = \" \".join(current_seed_tokens[1:])\n",
    "        \n",
    "    return output_text\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
