





import numpy as np








def sigmoid(x):
    return 1.0 / (1.0 + np.exp(-x))

def sigmoid_derivative(x):
    s = sigmoid(x)
    return s * (1 - s)









class Neuron:
    def __init__(self, n_inputs, weight_scale = 0.1):
        self.w = np.random.randn(n_inputs) * weight_scale
        self.b = 0.0
        
    def forward(self, x: np.ndarray):
        z = float(np.dot(self.w, x) + self.b)
        return sigmoid(z)














class Layer:
    def __init__(self, n_inputs, n_neurons, activation, activation_derivative):
        self.n_inputs = n_inputs
        self.n_neurons = n_neurons
        self.activation = activation
        self.activation_derivative = activation_derivative
        
        # 1
        # self.W = np.random.randn(n_neurons, n_inputs) * np.sqrt(2 / n_inputs)
        # self.b = np.zeros((n_neurons, 1))
        
        # 2
        self.W = np.random.randn(n_inputs, n_neurons) * np.sqrt(2 / n_inputs)
        self.b = np.zeros((1, n_neurons))
        
        # Кэш для обратного распространения
        self.A_prev = None
        self.Z = None
        self.A = None
    
    def forward(self, A_prev):
        self.A_prev = A_prev
        # 1
        # self.Z = self.W @ A_prev + self.b # размерность (n_neurons, m)
        # 2
        self.Z = self.A_prev @ self.W + self.b # размерность (m, n_neurons)
        self.A = self.activation(self.Z)
        return self.A
        





def mse(Y_pred, Y):
    return np.mean((Y_pred - Y)**2)

def mse_derivative(Y_pred, Y):
    return 2*(Y_pred - Y)


def bce(Y_pred, Y):
    eps = 1e-8
    return -np.mean(Y * np.log(Y_pred + eps) + (1 - Y) * np.log(1 - Y_pred + eps))

def bce_derivative(Y_pred, Y):
    eps = 1e-8
    return -(Y / (Y_pred + eps)) + ((1 - Y) / (1 - Y_pred + eps))





class NeuralNetwork:
    def __init__(self, n_inputs, learning_rate, epochs, loss_func, loss_func_derivative):
        self.n_inputs = n_inputs
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.layer_sizes = [n_inputs]
        self.layers = []
        self.loss_func = loss_func
        self.loss_func_derivative = loss_func_derivative
        
    def addLayer(self, n_neurons, activation, activation_derivative):
        layer = Layer(self.layer_sizes[-1], n_neurons, activation, activation_derivative)
        self.layers.append(layer)
        self.layer_sizes.append(n_neurons)

    def forward(self, X):
        A = X
        for layer in self.layers:
            A = layer.forward(A)
        return A

    def backward(self, Y_pred, Y):
        m = len(Y)
        dA = self.loss_func_derivative(Y_pred, Y)
        
        for i, layer in enumerate(reversed(self.layers)):
            # dL/dZ
            # для последнего слоя, если применять в конце softmax
            if i == 0 and layer.activation_derivative is None:
                dZ = dA
            else:
                dZ = dA * layer.activation_derivative(layer.Z)

            # dL/dW и dL/db
            # dW = (dZ @ layer.A_prev.T) / m 
            # db = np.sum(dZ, axis=1, keepdims=True) / m
            dW = (layer.A_prev.T @ dZ) / m # (n_inputs, m) @ (m, n_neurons)
            db = np.sum(dZ, axis=0, keepdims=True) / m

            # обновление dL/dA
            # dA = layer.W.T @ dZ
            dA = dZ @ layer.W.T # (m, n_neurons) @ (n_neurons, n_inputs)
            
            # градиентный спуск
            layer.W -= self.learning_rate * dW
            layer.b -= self.learning_rate * db

    def train(self, X, Y):
        if Y.ndim == 1:
            Y = Y.reshape(-1, 1)
        
        for i in range(self.epochs):
            Y_pred = self.forward(X)
            loss = self.loss_func(Y_pred, Y)
            self.backward(Y_pred, Y)
        
            if i%100 == 0:
                print("Epoch", i, "Loss:", loss)


    # добавил позже, чтобы применить для MNIST
    def train_v2(self, X, Y, batch_size=64, shuffle=True):
        if Y.ndim == 1:
            Y = Y.reshape(-1, 1)
    
        n_samples = X.shape[0]
        for epoch in range(self.epochs):
            if shuffle:
                indices = np.random.permutation(n_samples)
                X = X[indices]
                Y = Y[indices]
    
            epoch_loss = 0
    
            for start in range(0, n_samples, batch_size):
                end = start + batch_size
    
                X_batch = X[start:end]
                Y_batch = Y[start:end]
    
                Y_pred = self.forward(X_batch)
                loss = self.loss_func(Y_pred, Y_batch)
                self.backward(Y_pred, Y_batch)
    
                epoch_loss += loss
    
            epoch_loss /= (n_samples // batch_size)
    
            if epoch % 1 == 0:
                print(f"Epoch {epoch}, Loss: {epoch_loss:.4f}")

    
    def predict(self, X):
        return self.forward(X)
    








X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
Y = np.array([0, 1, 1, 0])

nn = NeuralNetwork(
    n_inputs=2, 
    learning_rate=0.5, 
    epochs=2000,
    loss_func=bce,
    loss_func_derivative=bce_derivative
)
nn.addLayer(2, activation=sigmoid, activation_derivative=sigmoid_derivative)
nn.addLayer(1, activation=sigmoid, activation_derivative=sigmoid_derivative)

nn.train(X, Y)
print(nn.predict(X))






import matplotlib.pyplot as plt 
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix


iris = load_iris()
X = iris.data
Y = iris.target





scaler = StandardScaler()
X = scaler.fit_transform(X)











Y = (Y == 1).astype(int)





X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y,
    test_size=0.3,
    random_state=42,
)








iris_nn_one = NeuralNetwork(
    n_inputs=4,
    learning_rate=0.1,
    epochs=5000,
    loss_func=bce,
    loss_func_derivative=bce_derivative
)

iris_nn_one.addLayer(
    1,
    activation=sigmoid,
    activation_derivative=sigmoid_derivative
)

iris_nn_one.train(X_train, Y_train)


Y_pred = iris_nn_one.predict(X_test)
Y_hat = (Y_pred > 0.5).astype(int)

print(confusion_matrix(Y_test.flatten(), Y_hat.flatten()))
print(classification_report(Y_test.flatten(), Y_hat.flatten()))











iris_nn_10 = NeuralNetwork(
    n_inputs=4,
    learning_rate=0.1,
    epochs=4000,
    loss_func=bce,
    loss_func_derivative=bce_derivative
)

iris_nn_10.addLayer(10, sigmoid, sigmoid_derivative)
iris_nn_10.addLayer(10, sigmoid, sigmoid_derivative)
iris_nn_10.addLayer(1, sigmoid, sigmoid_derivative)

iris_nn_10.train(X_train, Y_train)



Y_pred = iris_nn_10.predict(X_test)
Y_hat = (Y_pred > 0.5).astype(int)

print(confusion_matrix(Y_test.flatten(), Y_hat.flatten()))
print(classification_report(Y_test.flatten(), Y_hat.flatten()))








def plot_decision_boundary(model, X, Y, feature_idx=(0, 1), title=""):
    i, j = feature_idx

    # Диапазоны
    x_min, x_max = X[:, i].min() - 1, X[:, i].max() + 1
    y_min, y_max = X[:, j].min() - 1, X[:, j].max() + 1

    xx, yy = np.meshgrid(
        np.linspace(x_min, x_max, 300),
        np.linspace(y_min, y_max, 300)
    )

    # Подготовка входа для модели
    X_grid = np.zeros((xx.size, X.shape[1]))

    X_grid[:, i] = xx.ravel()
    X_grid[:, j] = yy.ravel()

    # Предсказания
    Z = model.predict(X_grid)
    Z = Z.reshape(xx.shape)

    # Отрисовка
    plt.figure(figsize=(7, 6))

    plt.contourf(
        xx, yy, Z > 0.5,
        alpha=0.3
    )

    plt.scatter(
        X[Y == 0, i],
        X[Y == 0, j],
        c="blue",
        label="class 0",
        edgecolor="k"
    )

    plt.scatter(
        X[Y == 1, i],
        X[Y == 1, j],
        c="red",
        label="class 1",
        edgecolor="k"
    )

    plt.xlabel(f"feature {i}")
    plt.ylabel(f"feature {j}")
    plt.title(title)
    plt.legend()
    plt.show()





feature_idx = (0, 2)
plot_decision_boundary(iris_nn_one, X, Y, feature_idx=feature_idx, title="Decision boundary — 1 neuron")
plot_decision_boundary(iris_nn_10, X, Y, feature_idx=feature_idx, title="Decision boundary — 2 hidden layers (10, 10)")





import plotly.graph_objects as go


def plot_decision_boundary_3d_plotly(
    model,
    X,
    Y,
    feature_idx=(0, 1, 2),
    grid_size=35,
    eps=0.02,
    title=""
):
    i, j, k = feature_idx

    x_min, x_max = X[:, i].min() - 1, X[:, i].max() + 1
    y_min, y_max = X[:, j].min() - 1, X[:, j].max() + 1
    z_min, z_max = X[:, k].min() - 1, X[:, k].max() + 1

    xx, yy, zz = np.meshgrid(
        np.linspace(x_min, x_max, grid_size),
        np.linspace(y_min, y_max, grid_size),
        np.linspace(z_min, z_max, grid_size)
    )

    X_grid = np.zeros((xx.size, X.shape[1]))

    X_grid[:, i] = xx.ravel()
    X_grid[:, j] = yy.ravel()
    X_grid[:, k] = zz.ravel()

    preds = model.predict(X_grid).reshape(xx.shape)

    mask = np.abs(preds - 0.5) < eps

    fig = go.Figure()

    # class 0
    fig.add_trace(go.Scatter3d(
        x=X[Y == 0, i],
        y=X[Y == 0, j],
        z=X[Y == 0, k],
        mode="markers",
        marker=dict(size=4, color="blue"),
        name="class 0"
    ))

    # class 1
    fig.add_trace(go.Scatter3d(
        x=X[Y == 1, i],
        y=X[Y == 1, j],
        z=X[Y == 1, k],
        mode="markers",
        marker=dict(size=4, color="red"),
        name="class 1"
    ))

    # decision boundary
    fig.add_trace(go.Scatter3d(
        x=xx[mask],
        y=yy[mask],
        z=zz[mask],
        mode="markers",
        marker=dict(size=2, color="green", opacity=0.4),
        name="decision boundary (p≈0.5)"
    ))

    fig.update_layout(
        title=title,
        scene=dict(
            xaxis_title=f"feature {i}",
            yaxis_title=f"feature {j}",
            zaxis_title=f"feature {k}",
        ),
        width=800,
        height=600
    )

    fig.show()



feature_idx = (1, 2, 3)

# plot_decision_boundary_3d_plotly(
#     iris_nn_one,
#     X,
#     Y,
#     feature_idx=feature_idx,
#     title="1 neuron"
# )

# plot_decision_boundary_3d_plotly(
#     iris_nn_10,
#     X,
#     Y,
#     feature_idx=feature_idx,
#     title="2 hidden layers (10,10)"
# )

















from sklearn.datasets import fetch_openml


X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)
X = X / 255.0
y = y.astype(int)


def softmax(Z):
    Z_shifted = Z - np.max(Z, axis=1, keepdims=True)  # числовая стабильность
    expZ = np.exp(Z_shifted)
    return expZ / np.sum(expZ, axis=1, keepdims=True)
    
def cce(Y_pred, Y_true):
    eps = 1e-9
    Y_pred = np.clip(Y_pred, eps, 1 - eps)
    return -np.mean(np.sum(Y_true * np.log(Y_pred), axis=1))

def cce_derivative(Y_pred, Y_true):
    return Y_pred - Y_true

def relu(Z):
    return np.maximum(0, Z)

def relu_derivative(Z):
    return (Z > 0).astype(float)



mnist_nn = NeuralNetwork(
    n_inputs=784,
    learning_rate=0.01,
    epochs=100,
    loss_func=cce,
    loss_func_derivative=cce_derivative
)


mnist_nn.addLayer(16, relu, relu_derivative)
mnist_nn.addLayer(16, relu, relu_derivative)
mnist_nn.addLayer(10, softmax, None)


Y = np.eye(10)[y]
Y


X_train, X_test, Y_train, Y_test = train_test_split(
    X, Y, test_size=0.3, random_state=42
)


mnist_nn.train_v2(X_train, Y_train, batch_size=64, shuffle=True)


Y_pred = mnist_nn.predict(X_test)
y_pred_labels = np.argmax(Y_pred, axis=1)
y_true_labels = np.argmax(Y_test, axis=1)

accuracy = np.mean(y_pred_labels == y_true_labels)
print("Accuracy:", accuracy)
print(classification_report(y_pred_labels, y_true_labels)) 



