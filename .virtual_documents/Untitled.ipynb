





import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.utils import to_categorical
import nltk
from nltk.tokenize import word_tokenize

nltk.download('punkt_tab')


FILE_PATH = 'alice_in_wonderland.txt'


try:
    with open(FILE_PATH, 'r', encoding='utf-8') as f:
        text = f.read().lower()
except:
    print(f"Failed to open file")

print(f'Length: {len(text)}')


tokens = word_tokenize(text)
print("Tokens:", len(tokens))


tokenizer = Tokenizer(filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n')
tokenizer.fit_on_texts([tokens])
word_index = tokenizer.word_index
total_words = len(word_index) + 1


indexed_sequences = tokenizer.texts_to_sequences([tokens])[0]


input_sequences = []
for i in range(SEQUENCE_LENGTH, len(indexed_sequences)):
    # Последовательность из N слов (SEQUENCE_LENGTH)
    n_gram_sequence = indexed_sequences[i-SEQUENCE_LENGTH:i]
    # Добавление к X
    input_sequences.append(n_gram_sequence)


input_sequences = np.array(input_sequences)

# Разделение на X (входы) и Y (целевое слово)
X = input_sequences[:, :-1]
y = input_sequences[:, -1]


y = to_categorical(y, num_classes=total_words)


model = Sequential()
# Embedding Layer: преобразует целые числа (индексы слов) в плотные векторы
model.add(Embedding(total_words, 100, input_length=X.shape[1]))
# Слой LSTM: ядро модели для работы с последовательностями
model.add(LSTM(150, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(100))
model.add(Dropout(0.2))
# Dense Layer: выходной слой с активацией softmax для получения вероятности следующего слова
model.add(Dense(total_words, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])





model.fit(X, y, epochs=50, verbose=1)


def generate_text(seed_text, next_words):
    """Генерирует текст на основе начальной фразы."""
    # Преобразование словаря "индекс -> слово" для обратного преобразования
    reverse_word_index = dict(map(reversed, word_index.items()))
    
    output_text = seed_text
    current_seed = seed_text

    for _ in range(next_words):
        # Токенизация текущего сида и преобразование в последовательность чисел
        token_list = tokenizer.texts_to_sequences([word_tokenize(current_seed)])[0]
        # Паддинг до нужной длины
        token_list = pad_sequences([token_list], maxlen=X.shape[1], padding='pre')
        
        # Предсказание вероятностей следующего слова
        predicted_probs = model.predict(token_list, verbose=0)[0]
        # Выбор индекса слова с максимальной вероятностью (жадный поиск)
        # Для более интересной генерации можно использовать сэмплирование
        predicted_index = np.argmax(predicted_probs)
        
        # Преобразование индекса обратно в слово
        next_word = reverse_word_index.get(predicted_index, '')
        
        # Обновление сида и выходного текста
        current_seed = current_seed + " " + next_word
        output_text = output_text + " " + next_word
        
        # Обновление сида для следующей итерации (убираем первое слово)
        current_seed_tokens = current_seed.split()
        current_seed = " ".join(current_seed_tokens[1:])
        
    return output_text


