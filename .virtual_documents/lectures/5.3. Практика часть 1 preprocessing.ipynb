








# если у вас линукс / мак / collab или ещё какая-то среда, в которой работает wget, можно скачать данные так:
get_ipython().run_line_magic("%capture", "")
get_ipython().getoutput("wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv")
get_ipython().getoutput("wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv")


import pandas as pd # библиотека для удобной работы с датафреймами
import numpy as np # библиотека для удобной работы со списками и матрицами

# библиотека, где реализованы основные алгоритмы машинного обучения
from sklearn.metrics import * 
from sklearn.model_selection import train_test_split 
from sklearn.pipeline import Pipeline


get_ipython().getoutput("head positive.csv")





# загружаем положительные твиты
positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text'])
positive['label'] = ['positive'] * len(positive) # устанавливаем метки

# загружаем отрицательные твиты
negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])
negative['label'] = ['negative'] * len(negative) # устанавливаем метки

# соединяем вместе
df = positive.append(negative)





df.sample(5, random_state=40)





x_train, x_test, y_train, y_test = train_test_split(df.text, df.label)











from nltk import ngrams





sentence = 'Если б мне платили каждый раз'.split()
sentence





list(ngrams(sentence, 1)) # униграммы





list(ngrams(sentence, 2)) # биграммы


list(ngrams(sentence, 3)) # триграммы


list(ngrams(sentence, 5)) # ... пентаграммы?








from sklearn.linear_model import LogisticRegression # можно заменить на любимый классификатор
from sklearn.feature_extraction.text import CountVectorizer # модель "мешка слов", см. далее














vectorizer = CountVectorizer(ngram_range=(1, 1))





vectorized_x_train = vectorizer.fit_transform(x_train)








list(vectorizer.vocabulary_.items())[:10]





vectorized_x_train.shape





clf = LogisticRegression(random_state=42, max_iter=1000) # фиксируем random_state для воспроизводимости результатов
clf.fit(vectorized_x_train, y_train)





vectorized_x_test = vectorizer.transform(x_test)





pred = clf.predict(vectorized_x_test)
print(classification_report(y_test, pred))








# инициализируем векторайзер 
trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))

# обучаем его и сразу применяем к x_train
trigram_vectorized_x_train = trigram_vectorizer.fit_transform(x_train)

# инициализируем и обучаем классификатор
clf = LogisticRegression(random_state=42, max_iter=1000)
clf.fit(trigram_vectorized_x_train, y_train)

# применяем обученный векторизатор к тестовым данным
trigram_vectorized_x_test = trigram_vectorizer.transform(x_test)

# получаем предсказания и выводим информацию о качестве
pred = clf.predict(trigram_vectorized_x_test)
print(classification_report(y_test, pred))











from sklearn.feature_extraction.text import TfidfVectorizer





# инициализируем векторизатор, в качестве переменных используем униграммы
tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 5))

# обучаем его и сразу применяем к x_train
tfidf_vectorized_x_train = tfidf_vectorizer.fit_transform(x_train)

# инициализируем и обучаем классификатор
clf = LogisticRegression(random_state=42)
clf.fit(tfidf_vectorized_x_train, y_train)

# применяем обученный векторизатор к тестовым данным
tfidf_vectorized_x_test = tfidf_vectorizer.transform(x_test)

# получаем предсказания и выводим информацию о качестве
pred = clf.predict(tfidf_vectorized_x_test)
print(classification_report(y_test, pred))








import nltk # уже знакомая нам библиотека nltk
from nltk.tokenize import word_tokenize # готовый токенизатор библиотеки nltk





nltk.download('punkt')
nltk.download('stopwords')





example = 'Но не каждый хочет что-то исправлять:('
word_tokenize(example)





example.split()





from nltk import tokenize
dir(tokenize)[:16]





wh_tok = tokenize.WhitespaceTokenizer()
list(wh_tok.span_tokenize(example))





tokenize.TreebankWordTokenizer().tokenize("don't stop me")





tokenize.SExprTokenizer().tokenize("(a (b c)) d e (f)")








# импортируем стоп-слова из библиотеки nltk
from nltk.corpus import stopwords

# посмотрим на стоп-слова для русского языка
print(stopwords.words('russian'))





from string import punctuation
punctuation





noise = stopwords.words('russian') + list(punctuation)





# инициализируем умный векторайзер 
smart_vectorizer = CountVectorizer(ngram_range=(1, 1), 
                                   tokenizer=word_tokenize, 
                                   stop_words=noise)


# обучаем его и сразу применяем к x_train
smart_vectorized_x_train = smart_vectorizer.fit_transform(x_train)

# инициализируем и обучаем классификатор
clf = LogisticRegression(random_state=42)
clf.fit(smart_vectorized_x_train, y_train)

# применяем обученный векторайзер к тестовым данным
smart_vectorized_x_test = smart_vectorizer.transform(x_test)

# получаем предсказания и выводим информацию о качестве
pred = clf.predict(smart_vectorized_x_test)
print(classification_report(y_test, pred))











# устанавливаем pymorphy2
get_ipython().getoutput("pip install pymorphy2")





from pymorphy2 import MorphAnalyzer
pymorphy2_analyzer = MorphAnalyzer()





sent = ['Если', 'б', 'мне', 'платили', 'каждый', 'раз']
sent





ana = pymorphy2_analyzer.parse(sent[3])
ana





ana[0].normal_form








# инициализируем умный векторайзер stop-words НЕ ИСПОЛЬЗУЕМ!
alternative_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), 
                                               tokenizer=word_tokenize)

# обучаем его и сразу применяем к x_train
alternative_tfidf_vectorized_x_train = alternative_tfidf_vectorizer.fit_transform(x_train)

# инициализируем и обучаем классификатор
clf = LogisticRegression(random_state=42)
clf.fit(alternative_tfidf_vectorized_x_train, y_train)

# применяем обученный векторайзер к тестовым данным
alternative_tfidf_vectorized_x_test = alternative_tfidf_vectorizer.transform(x_test)

# получаем предсказания и выводим информацию о качестве
pred = clf.predict(alternative_tfidf_vectorized_x_test)
print(classification_report(y_test, pred))








cool_token = ')'
pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]
print(classification_report(pred, y_test))





# инициализируем векторайзер для символов
char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 1))

# обучаем его и сразу применяем к x_train
char_vectorized_x_train = char_vectorizer.fit_transform(x_train)

# инициализируем и обучаем классификатор
clf = LogisticRegression(random_state=42)
clf.fit(char_vectorized_x_train, y_train)

# применяем обученный векторайзер к тестовым данным
char_vectorized_x_test = char_vectorizer.transform(x_test)

# получаем предсказания и выводим информацию о качестве
pred = clf.predict(char_vectorized_x_test)
print(classification_report(y_test, pred))








import re











result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') 
print(result)








result = re.split(',', 'itsy, bitsy, teenie, weenie') 
print(result)





result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit=2) 
print(result)





result = re.sub('a', 'b', 'abcabc')
print (result)





# Пример: построение списка всех слов строки:
prog = re.compile('[А-Яа-яё\-]+')
prog.findall("Слова? Да, больше, ещё больше слов! Что-то ещё.")
